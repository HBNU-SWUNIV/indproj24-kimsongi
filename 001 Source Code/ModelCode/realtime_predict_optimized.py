import asyncio
import cv2
import mediapipe as mp
import multiprocessing as mproc
import numpy as np
from keras.models import load_model
import joblib
from collections import deque
from livekit import rtc
from PIL import ImageFont, ImageDraw, Image
import queue
import sys
import threading
from livekit_auth import create_token

# ì„¤ì •ê°’
SERVER_URL = "ws://127.0.0.1:7880" # sfu ì„œë²„ ip ì£¼ì†Œ ëŒ€ì…
ACCESS_TOKEN = create_token("ksl_worker", "dev-room")
MODEL_PATH = "/Users/kyungrim/Library/CloudStorage/GoogleDrive-20221999@edu.hanbat.ac.kr/ë‚´ ë“œë¼ì´ë¸Œ/2025ìº¡ìŠ¤í†¤í”„ë¡œì íŠ¸/KSL_lstm/models/gesture_lstm_model_dual_v2.h5" # lstm ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ëŒ€ì…
ENCODER_PATH = "/Users/kyungrim/Library/CloudStorage/GoogleDrive-20221999@edu.hanbat.ac.kr/ë‚´ ë“œë¼ì´ë¸Œ/2025ìº¡ìŠ¤í†¤í”„ë¡œì íŠ¸/KSL_lstm/processed_lstm/label_encoder_lstm_dual.pkl" # preprocess/*.pkl íŒŒì¼ ê²½ë¡œ ëŒ€ì…
T5_MODEL_PATH = "/Users/kyungrim/Library/CloudStorage/GoogleDrive-20221999@edu.hanbat.ac.kr/ë‚´ ë“œë¼ì´ë¸Œ/2025ìº¡ìŠ¤í†¤í”„ë¡œì íŠ¸/KSL_t5/my_finetuned_t5_model"
FRAMES_PER_SEQUENCE = 30 
FONT_PATH = "C:/Windows/Fonts/malgun.ttf"
CONFIDENCE_THRESHOLD = 0.75
PREDICTION_INTERVAL = 3


prediction_result = ("", 0.0)
sentence_words = []
sentence_lock = threading.Lock()
generated_sentence = ""
is_predicting = False

# ëª¨ë¸ ë° ë¦¬ì†ŒìŠ¤ ë¡œë“œ
if mproc.current_process().name == "MainProcess":
    print("â–¶ ëª¨ë¸ê³¼ ë¦¬ì†ŒìŠ¤ ë¡œë“œ ì¤‘...")
    try:
        model = load_model(MODEL_PATH)
        label_encoder = joblib.load(ENCODER_PATH)
        print("âœ… LSTM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

    mp_hands = mp.solutions.hands
    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)
    mp_drawing = mp.solutions.drawing_utils

    frame_buffer = deque(maxlen=FRAMES_PER_SEQUENCE)


# í•œê¸€ í…ìŠ¤íŠ¸ ì¶œë ¥ í•¨ìˆ˜
_font_cache = {}
def draw_korean_text(img, text, position, font_size=32, color=(255, 255, 255), max_width=None):
    if font_size not in _font_cache:
        _font_cache[font_size] = ImageFont.truetype(FONT_PATH, font_size)
    font = _font_cache[font_size]

    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    x, y = position
    line_height = font_size + 5
    if max_width is None:
        max_width = img.shape[1] * 0.9 - x

    words = text.split(' ')
    current_line = []
    current_y = y

    for word in words:
        test_line = ' '.join(current_line + [word])
        bbox = font.getbbox(test_line)
        text_width = bbox[2] - bbox[0]
        if text_width < max_width:
            current_line.append(word)
        else:
            draw.text((x, current_y), ' '.join(current_line), font=font, fill=color)
            current_y += line_height
            current_line = [word]
    if current_line:
        draw.text((x, current_y), ' '.join(current_line), font=font, fill=color)

    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

# T5 ë¬¸ì¥ ìƒì„± í”„ë¡œì„¸ìŠ¤ í•¨ìˆ˜
def t5_worker(input_queue, output_queue, t5_model_path):
    import torch
    from transformers import T5ForConditionalGeneration, T5TokenizerFast as T5Tokenizer
    torch.set_num_threads(1)

    try:
        tokenizer = T5Tokenizer.from_pretrained(t5_model_path, local_files_only=True)
        t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_path, local_files_only=True)
        print("âœ… T5 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

    while True:
        words = input_queue.get()
        prompt = f"ë¬¸ì¥ ìƒì„±: {', '.join(words)}"
        print(f"ğŸ“ T5 ì…ë ¥: '{prompt}'")

        inputs = tokenizer(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = t5_model.generate(
                inputs.input_ids, max_length=64, num_beams=5, early_stopping=True,
                repetition_penalty=2.0, no_repeat_ngram_size=2
            )
        result_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)
        output_queue.put(result_sentence)
        print(f"âœ… T5 ìƒì„± ë¬¸ì¥: {result_sentence}")

# LSTM ì œìŠ¤ì²˜ ì˜ˆì¸¡ ìŠ¤ë ˆë“œ í•¨ìˆ˜
def predict_gesture(sequence_data):
    global prediction_result, is_predicting, sentence_words, generated_sentence
    
    prediction = model.predict(sequence_data, verbose=0)
    confidence = np.max(prediction)
    predicted_index = np.argmax(prediction)
    predicted_label = label_encoder.inverse_transform([predicted_index])[0]
    
    prediction_result = (predicted_label, confidence)
    
    if confidence >= CONFIDENCE_THRESHOLD and (not sentence_words or sentence_words[-1] != predicted_label):
        if predicted_label == "OK":
            if sentence_words:
                t5_input_queue.put(list(sentence_words))
                sentence_words.clear()
        else:
            generated_sentence = ""
            sentence_words.append(predicted_label)
            print(f"â• ë‹¨ì–´ ì¶”ê°€: {predicted_label} (í˜„ì¬ ë¦¬ìŠ¤íŠ¸: {sentence_words})")
            
    is_predicting = False

# --- LiveKit ë£¨í”„ ---
room = None
frame_queue = queue.Queue(maxsize=1)
livekit_task = None
connection_failed = threading.Event()
sentence_queue = asyncio.Queue()
livekit_loop = None

async def receive_from_livekit():
    global room
    room = rtc.Room()

    async def receive_frames(stream):
        async for event in stream:
            converted_frame = event.frame.convert(rtc.VideoBufferType.RGB24)
            image = np.frombuffer(converted_frame.data, dtype=np.uint8)
            image = image.reshape((converted_frame.height, converted_frame.width, 3))
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

            if frame_queue.full():
                frame_queue.get_nowait()
            frame_queue.put_nowait(image)

    @room.on("track_published")
    def on_track_published(publication, participant):
        if participant.identity == "ksl" and publication.kind == rtc.TrackKind.KIND_VIDEO:
            publication.set_subscribed(True)

    @room.on("track_subscribed")
    def on_track_subscribed(track, publication, participant):
        global livekit_task
        if track.kind == rtc.TrackKind.KIND_VIDEO:
            video_stream = rtc.VideoStream(track)
            livekit_task = asyncio.create_task(receive_frames(video_stream))

    @room.on("participant_disconnected")
    def on_participant_disconnected(participant):
        global livekit_task
        if participant.identity == "ksl" and livekit_task is not None:
            livekit_task.cancel()
            livekit_task = None

    @room.on("disconnected")
    def on_disconnected(reason):
        connection_failed.set()

    try:
        await room.connect(SERVER_URL, ACCESS_TOKEN, rtc.RoomOptions(auto_subscribe=False))
    except rtc.ConnectError:
        connection_failed.set()
        return

    if "ksl" in room.remote_participants:
        for publication in room.remote_participants["ksl"].track_publications.values():
            if publication.kind == rtc.TrackKind.KIND_VIDEO:
                publication.set_subscribed(True)
                break

    asyncio.create_task(send_message_loop(room))
    await asyncio.Future()

async def send_message_loop(room):
    while True:
        sentence = await sentence_queue.get()
        data = sentence.encode()

        await room.local_participant.publish_data(data, destination_identities=["asl"])

def run_livekit_background():
    global livekit_loop
    livekit_loop = asyncio.new_event_loop()
    asyncio.set_event_loop(livekit_loop)
    livekit_loop.run_until_complete(receive_from_livekit())

# --- ë©”ì¸ ë£¨í”„ ---
if __name__ == "__main__":
    mproc.freeze_support()
    mproc.set_start_method("spawn", force=True)

    t5_input_queue = mproc.Queue()
    t5_output_queue = mproc.Queue()
    mproc.Process(target=t5_worker, args=(t5_input_queue, t5_output_queue, T5_MODEL_PATH), daemon=True).start()

    mode = int(input("ëª¨ë“œ ì„ íƒ (1: ë¡œì»¬ ì¹´ë©”ë¼, 2: ì„œë²„ ëª¨ë‹ˆí„°ë§) - "))
    if not 1 <= mode <= 2:
        print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤.")
        sys.exit(1)

    if mode == 1:
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); sys.exit(1)
    elif mode == 2:
        threading.Thread(target=run_livekit_background, daemon=True).start()

    print("â–¶ ì‹¤ì‹œê°„ ìˆ˜ì–´ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤. ('q': ì¢…ë£Œ, 'ìŠ¤í˜ì´ìŠ¤ë°”': ì´ˆê¸°í™”)")
    frame_count = 0
    last_frame = np.zeros((640, 480, 3), dtype=np.uint8)

    while True:
        if mode == 1:
            if not cap.isOpened(): break
            ret, frame = cap.read()
            if not ret: break
        elif mode == 2:
            if connection_failed.is_set():
                print("SFU ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break

            try:
                frame = frame_queue.get(timeout=0.1)
                last_frame = frame
            except queue.Empty:
                frame = last_frame

        frame = cv2.flip(frame, 1)
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(rgb_frame)

        if results.multi_hand_landmarks and results.multi_handedness:
            hand_data = {"Left": [], "Right": []}
            for hand_landmarks, hand_handedness in zip(results.multi_hand_landmarks, results.multi_handedness):
                hand_label = hand_handedness.classification[0].label
                hand_data[hand_label] = [lm for lm in hand_landmarks.landmark]

            normalized_left = [0.0] * 63
            if hand_data["Left"]:
                left_wrist = hand_data["Left"][0]
                for i, lm in enumerate(hand_data["Left"]):
                    normalized_left[i*3:i*3+3] = [lm.x - left_wrist.x, lm.y - left_wrist.y, lm.z - left_wrist.z]

            normalized_right = [0.0] * 63
            if hand_data["Right"]:
                right_wrist = hand_data["Right"][0]
                for i, lm in enumerate(hand_data["Right"]):
                    normalized_right[i*3:i*3+3] = [lm.x - right_wrist.x, lm.y - right_wrist.y, lm.z - right_wrist.z]
            
            one_frame = normalized_left + normalized_right
            frame_buffer.append(one_frame)
        else:
            frame_buffer.clear()

        frame_count += 1
        if len(frame_buffer) == FRAMES_PER_SEQUENCE and not is_predicting and frame_count % PREDICTION_INTERVAL == 0:
            is_predicting = True
            sequence_data = np.array(frame_buffer).reshape(1, FRAMES_PER_SEQUENCE, 126)
            threading.Thread(target=predict_gesture, args=(sequence_data,), daemon=True).start()

        try:
            current_sentence = t5_output_queue.get_nowait()
            if mode == 2:
                asyncio.run_coroutine_threadsafe(sentence_queue.put(current_sentence), livekit_loop)
            generated_sentence = current_sentence
        except queue.Empty:
            current_sentence = generated_sentence

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
                
        label, conf = prediction_result
        feedback_text = f"Guess: {label} ({conf:.2f})"
        color = (0, 255, 0) if conf >= CONFIDENCE_THRESHOLD else (255, 0, 0)
        frame = draw_korean_text(frame, feedback_text, (10, 30), font_size=32, color=color)

        with sentence_lock:
            words_text = " ".join(sentence_words)
        frame = draw_korean_text(frame, f"ì…ë ¥: {words_text}", (10, 80), font_size=40, color=(255, 235, 59), max_width=frame.shape[1] - 20)
        
        if current_sentence:
            frame = draw_korean_text(frame, f"ê²°ê³¼: {current_sentence}", (10, 130), font_size=40, color=(129, 212, 250), max_width=frame.shape[1] - 20)

        cv2.imshow("KSL Translator", frame)
        key = cv2.waitKey(1) & 0xFF

        if key == ord('q'):
            break
        elif key == ord(' '):
            sentence_words.clear()
            generated_sentence = ""
            prediction_result = ("", 0.0)
            print("ğŸ”„ ë¬¸ì¥ ë° ë‹¨ì–´ ëª©ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    if mode == 1:
        cap.release()
    elif mode == 2 and room is not None:
        asyncio.run_coroutine_threadsafe(room.disconnect(), livekit_loop).result()
    cv2.destroyAllWindows()

