# test_model.py
from transformers import T5ForConditionalGeneration, T5TokenizerFast

MODEL_PATH = MODEL_PATH = "/Users/kyungrim/Library/CloudStorage/GoogleDrive-20221999@edu.hanbat.ac.kr/ë‚´ ë“œë¼ì´ë¸Œ/2025ìº¡ìŠ¤í†¤í”„ë¡œì íŠ¸/my_finetuned_t5_model"

try:
    tokenizer = T5TokenizerFast.from_pretrained(MODEL_PATH)
    model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH)
    print("âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("ğŸ‘‰ 1~3ë‹¨ê³„ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”. íŠ¹íˆ ì´ì „ í´ë” ì‚­ì œì™€ ì¬í•™ìŠµì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    exit()

prompt = ""
print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì…ë ¥: '{prompt}'")

# í† í°í™” ë° ë¬¸ì¥ ìƒì„±
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(
    inputs.input_ids,
    max_length=64,
    num_beams=5,
    early_stopping=True
)

# ê²°ê³¼ í™•ì¸
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"âœ… ëª¨ë¸ ìƒì„± ë¬¸ì¥: {result}")