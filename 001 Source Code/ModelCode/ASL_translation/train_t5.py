import os
import json
import math
import random
from collections import defaultdict

import numpy as np
import torch
import matplotlib.pyplot as plt

from datasets import Dataset
from transformers import (
    T5ForConditionalGeneration,
    T5TokenizerFast as T5Tokenizer,
    TrainingArguments,
    Trainer,
    TrainerCallback,
    DataCollatorForSeq2Seq,
)


SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)


MODEL_NAME = "paust/pko-t5-base"
DATASET_PATH = "./drive/MyDrive/2025ìº¡ìŠ¤í†¤í”„ë¡œì íŠ¸/train_dataset.json"

SAVE_DIR = "./drive/MyDrive/2025ìº¡ìŠ¤í†¤í”„ë¡œì íŠ¸"
SAVE_PATH = os.path.join(SAVE_DIR, "my_finetuned_t5_model")
LOG_DIR = "./logs"

os.makedirs(SAVE_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

print("â–¶ ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ")
model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)
tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)

model.config.dropout_rate = 0.1
model.config.attention_dropout_rate = 0.1


print("â–¶ ë°ì´í„° ë¡œë“œ:", DATASET_PATH)
with open(DATASET_PATH, "r", encoding="utf-8") as f:
    raw_data = json.load(f)


np.random.shuffle(raw_data)
split_point = int(len(raw_data) * 0.9)
train_data = raw_data[:split_point]
eval_data = raw_data[split_point:]

print(f"í›ˆë ¨ ë°ì´í„°: {len(train_data)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(eval_data)}ê°œ")

# --- Train/Eval ì…ë ¥ ì¤‘ë³µ ì ê²€ ---
train_inputs = set(ex["input"] for ex in train_data)
eval_inputs = set(ex["input"] for ex in eval_data)
overlap = sorted(list(train_inputs & eval_inputs))
print(f"[CHECK] Train/Eval ì¤‘ë³µ ì…ë ¥ ê°œìˆ˜: {len(overlap)}")
if len(overlap) > 0:
    print("  ì˜ˆì‹œ ì¤‘ë³µ ì…ë ¥(ìµœëŒ€ 10ê°œ):", overlap[:10])

# í•„ìš” ì‹œ: ì¤‘ë³µ ì œê±°
eval_data = [ex for ex in eval_data if ex["input"] not in train_inputs]
print(f"[FIX] ì¤‘ë³µ ì œê±° í›„ ê²€ì¦ ë°ì´í„°: {len(eval_data)}ê°œ")

train_dataset = Dataset.from_list(train_data)
eval_dataset = Dataset.from_list(eval_data)


def preprocess_function(examples):
    # ì…ë ¥
    model_inputs = tokenizer(
        examples["input"],
        max_length=128,
        truncation=True,
        padding="max_length"
    )
    # íƒ€ê¹ƒ (ìƒˆ API: text_target=)
    labels = tokenizer(
        text_target=examples["output"],
        max_length=128,
        truncation=True,
        padding="max_length"
    )
    # pad í† í°ì„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ (loss ì œì™¸)
    pad_id = tokenizer.pad_token_id
    masked_labels = []
    for seq in labels["input_ids"]:
        masked_labels.append([tid if tid != pad_id else -100 for tid in seq])
    model_inputs["labels"] = masked_labels
    return model_inputs

print("â–¶ ì „ì²˜ë¦¬ ì‹œì‘")
tokenized_train_dataset = train_dataset.map(
    preprocess_function, batched=True, remove_columns=train_dataset.column_names
)
tokenized_eval_dataset = eval_dataset.map(
    preprocess_function, batched=True, remove_columns=eval_dataset.column_names
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)


class LossRecorder(TrainerCallback):
    def __init__(self):
        # (epoch_float, loss)
        self.train_logs = []
        self.eval_logs = []

    def on_log(self, args, state, control, **kwargs):
        logs = kwargs.get("logs", {})
        if "loss" in logs and state.epoch is not None:
            # step-ë‹¨ìœ„ train loss
            self.train_logs.append((float(state.epoch), float(logs["loss"])))

    def on_evaluate(self, args, state, control, metrics, **kwargs):
        if "eval_loss" in metrics and state.epoch is not None:
            # epoch-ë‹¨ìœ„ eval loss
            self.eval_logs.append((float(state.epoch), float(metrics["eval_loss"])))

    def get_epochwise_losses(self, agg="mean"):
        # ì—í­ë³„ train loss ì§‘ê³„ & eval loss ë§¤í•‘
        bucket = defaultdict(list)
        for ep, l in self.train_logs:
            bucket[math.floor(ep)].append(l)

        train_epoch_loss = {}
        for ep in sorted(bucket.keys()):
            vals = bucket[ep]
            if agg == "last":
                train_epoch_loss[ep] = vals[-1]
            else:
                train_epoch_loss[ep] = sum(vals) / len(vals)

        eval_epoch_loss = {}
        for ep, l in self.eval_logs:
            eval_epoch_loss[math.floor(ep)] = l

        return train_epoch_loss, eval_epoch_loss

loss_recorder = LossRecorder()

@torch.no_grad()
def compute_exact_match(trainer: Trainer, eval_ds: Dataset, limit_samples: int = None, num_beams: int = 4):
    model.eval()
    dataloader = trainer.get_eval_dataloader(eval_ds)
    total, exact = 0, 0
    preds_all, refs_all = [], []

    for batch in dataloader:
        input_ids = batch["input_ids"].to(model.device)
        attention_mask = batch["attention_mask"].to(model.device)
        gen_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=128,
            num_beams=num_beams
        )
        pred_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)

        # ë¼ë²¨ ë³µì›(-100 -> pad_id) í›„ ë””ì½”ë“œ
        labels = batch["labels"].clone()
        labels = labels.cpu().numpy()
        labels = np.where(labels == -100, tokenizer.pad_token_id, labels)
        ref_text = tokenizer.batch_decode(labels, skip_special_tokens=True)

        for p, r in zip(pred_text, ref_text):
            preds_all.append(p.strip())
            refs_all.append(r.strip())
            exact += int(p.strip() == r.strip())
            total += 1

        if limit_samples is not None and total >= limit_samples:
            break

    em = exact / max(1, total)
    print(f"[GEN-EVAL] Exact Match: {em:.3f}  ({exact}/{total})")
    return em, preds_all, refs_all

MAX_EPOCHS = 10
early_stopping_patience = 3
best_eval_loss = float("inf")
patience_counter = 0


training_args = TrainingArguments(
    output_dir=os.path.join(SAVE_DIR, "t5_finetune_results"),
    num_train_epochs=1,                     
    per_device_train_batch_size=4,
    per_device_eval_batch_size=8,           
    learning_rate=3e-5,                     
    weight_decay=0.01,                      
    logging_dir='./logs',
    logging_steps=50,                       
    save_total_limit=2,                     
    save_steps=500,                         
    prediction_loss_only=False           
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,
    data_collator=data_collator,
    callbacks=[loss_recorder],
)


print("â–¶ ìˆ˜ë™ ì¡°ê¸° ì¢…ë£Œ ê¸°ëŠ¥ìœ¼ë¡œ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

for epoch in range(MAX_EPOCHS):
    print(f"--- Epoch {epoch + 1}/{MAX_EPOCHS} ---")

    trainer.train()

    eval_results = trainer.evaluate()
    current_eval_loss = float(eval_results["eval_loss"])
    print(f"Epoch {epoch + 1} - ê²€ì¦ ì†ì‹¤(Validation Loss): {current_eval_loss:.6f}")

    _ = compute_exact_match(trainer, tokenized_eval_dataset, limit_samples=None, num_beams=4)
    
    if current_eval_loss < best_eval_loss:
        best_eval_loss = current_eval_loss
        patience_counter = 0
        print(f"âœ… ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë°œê²¬! (eval_loss: {best_eval_loss:.6f})")
        print(f"   ëª¨ë¸ì„ '{SAVE_PATH}'ì— ì €ì¥í•©ë‹ˆë‹¤...")
        trainer.save_model(SAVE_PATH)
        tokenizer.save_pretrained(SAVE_PATH)
    else:
        patience_counter += 1
        print(f"   ì„±ëŠ¥ í–¥ìƒ ì—†ìŒ. (Patience: {patience_counter}/{early_stopping_patience})")

    # ì¡°ê¸° ì¢…ë£Œ
    if patience_counter >= early_stopping_patience:
        print(f"ğŸ”´ {early_stopping_patience}ë²ˆ ì—°ì†ìœ¼ë¡œ ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ ì•Šì•„ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

print("âœ… ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"âœ… ìµœì¢…ì ìœ¼ë¡œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ì´ '{SAVE_PATH}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")